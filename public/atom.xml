<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US">
    <title type="text">Data Maggie</title>
    <subtitle type="html">MemE is a powerful and highly customizable GoHugo theme for personal blogs.</subtitle>
    <updated>2020-01-04T19:53:25-05:00</updated>
    <id>/</id>
    <link rel="alternate" type="text/html" href="/" />
    <link rel="self" type="application/atom+xml" href="/atom.xml" />
    <author>
            <name>Maggie Church</name>
            <uri>/</uri>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    <generator uri="https://gohugo.io/" version="0.59.1">Hugo</generator>
        <entry>
            <title type="text">What to Know About Margin of Error for 2020</title>
            <link rel="alternate" type="text/html" href="/post/moe/" />
            <id>/post/moe/</id>
            <updated>2020-01-04T19:53:24-05:00</updated>
            <published>2019-11-19T00:00:00+00:00</published>
            <author>
                    <name>Maggie Church</name>
                    <uri>/</uri>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html"><![CDATA[Recently, my friend Daniel Stublen made the argument that reporters should always include the margin of error when citing polls. This led to a good discussion about what the margin of error really is, but I wanted to dive deeper into it, here.]]></summary>
            
                <content type="html"><![CDATA[


<p>Recently, my friend Daniel Stublen made the argument that reporters should always include the margin of error when citing polls. This led to a good discussion about what the margin of error really is, but I wanted to dive deeper into it, here.</p>
<div id="margin-of-error" class="section level2">
<h2>Margin of Error</h2>
<p>Polls are bound to have sampling error (the error that occurs because you ask a subset of the population, rather than the entire population). The margin of error measures the maximum amount by which the sample results are expected to differ from those of the actual population. The <strong>lower the margin</strong>, the <strong>more confident</strong> you can be in your estimation.</p>
<div id="the-formula" class="section level3">
<h3>The Formula</h3>
<p>Before I got into my thoughts on how margin of error should be reported, I want to review what margin of error really is (as painlessly as I can).</p>
<p>So the formula for the margin of error is as follows: <span class="math display">\[ z * \sqrt{\frac{(p)(1-p)}{n}} \]</span></p>
<p>To solve this, we really just need to know what n (sample size) is. That’s because there are some standard practices in the polling world, that turn the formula into this:</p>
<p><span class="math display">\[ = 1.96 * \sqrt{\frac{(.5)(.5)}{n}} \]</span></p>
<p>Why?</p>
<ul>
<li><p>The proportion of support p = .5: Pollsters will assume that any candidate’s chances are a toss-up (this, I just learned). 50-50 odds represents the upper bound of uncertainty, and yields the <a href="https://abcnews.go.com/PollingUnit/sampling-error-means/story?id=5984818">maximum margin of error</a>. Basically, they use 50-50 to be conservative in their estimation.</p></li>
<li><p>Confidence level indicator z = 1.96: It’s conventional to use a 95% confidence level, so z = 1.96. Look up z scores if you want to understand why. You <em>could</em> use a larger confidence level, say 99%, but then your margin of error will be larger.</p></li>
<li><p>Number of respondents n: This what pollsters can really control. All else equal, more respondents will lower the margin of error. The tricky thing with polling is that you want your sample to be demographically representative of the voting population. Also, you want to capture likely voters, because only about <a href="https://fivethirtyeight.com/features/no-voter-turnout-wasnt-way-down-from-2012/">60% of elegible voters</a> actually do so.</p></li>
</ul>
<p>Let’s consider a real-world example. Between October 25-30, 2019, the New York Times Upshot/Siena College asked 439 Iowans “Who is your first choice?” and 22% responded with Elizabeth Warren.</p>
<p><span class="math display">\[ 1.96 * \sqrt{\frac{(.5)(.5)}{439}} \]</span> <span class="math display">\[ = 4.7\% \]</span> </br></p>
<p><img src="/post/moe_files/figure-html/unnamed-chunk-1-1.png" width="288" /></p>
<p>So the interpretation of this poll is: “At least 95 out of 100 times, <span class="math inline">\(22 \% \pm 4.7\%\)</span> of Iowa’s democrats will support Warren.” But keep in mind, 22% is still the most likely outcome given our sample!</p>
</div>
<div id="how-to-report-the-horse-race" class="section level3">
<h3>How to Report the Horse Race</h3>
<p>Here are the full results of the poll:</p>
<p><img src="/post/moe_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Here are three possible ways to report these results:</p>
<p>“Warren is polling ahead of Klobuchar.”</p>
<ul>
<li>I think this is totally okay to say, without caveats. Because the difference in the two candidates’ support is <em>greater</em> than their margins (greater than a 9.2-point lead, in this case), Warren’s lead is probably not due to sampling error.</li>
</ul>
<p>“Warren is polling ahead of Biden.”</p>
<ul>
<li>Eh, I think this is sketchy. Yes, Warren is polling ahead, but that lead could be due to sampling error.</li>
</ul>
<p>“Warren and Biden are in a statistical tie.”</p>
<ul>
<li>I think this take is an exaggeration. Sure, her lead on him is less than 9.2 points, but that number comes from the 95% confidence level. What if we use a 70% confidence level? Then, using the corresponding z-score, our margin of error would be…</li>
</ul>
<p><span class="math display">\[ 1.04 * \sqrt{\frac{(.5)(.5)}{439}} \]</span> <span class="math display">\[ = 2.5\% \]</span></p>
<div style="white-space: pre-line;">       …and the graph would look more like this:</div>
<p><img src="/post/moe_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div style="white-space: pre-line;">       Now, with 70% confidence, Warren’s lead over Biden is
       significant.</div>
</div>
</div>
<div id="the-verdict" class="section level2">
<h2>The Verdict</h2>
<p>I agree with Daniel that it is wise for the media to mention margin of error, especially when the race is a close one.</p>
<p>The Times/Sienna College poll that I’ve been using throughout this post was used in a <a href="https://www.nytimes.com/2019/11/01/us/politics/iowa-poll-warren-biden.html">nytimes piece</a> a couple of days after the poll was conducted. Here’s how they chose to visualize the results:</p>
<div class="figure">
<img src="https://i.imgur.com/ppEyQ9m.jpg" />

</div>
<p>This figure was one of the first things you read in the article. I think its format is the norm for poll visualizations: percentages. I would prefer to see the type of graph that I created above: percentages <em>with</em> error bars. The margin of error isn’t even mentioned until about halfway down the article, and it’s never interpreted.</p>
<div class="figure">
<img src="https://i.imgur.com/xaTQN9m.jpg" />

</div>
<p>Honestly, I don’t know the extent to which voters are mislead by an article like this, and I don’t know the extent to which polls effect voting behavior. Well, I do know that there can be a <a href="http://researchdmr.com/Why_Are_Polls_Self_Fulfilling_Prophecies">bandwagon effect</a> that makes poll results a bit self-fulfilling. But those questions are beasts of which I won’t take on here.</p>
<p>The bottom line is: let’s all be data literate around election time, and remember what the margin of error means!</p>
</div>
<div id="why-do-pollsters-get-different-numbers" class="section level2">
<h2>Why do pollsters get different numbers?</h2>
<p>If different polls are reporting different results, and the difference is greater than the sum of their respective margins of error, this difference is probably not due to chance. The difference is probably structural, and has to do with <em>who</em> they sampled. Here are some of the more consequential variations in polling methods:</p>
<p><strong>Polling method.</strong> Pollsters select respondents by either:</p>
<ol style="list-style-type: decimal">
<li>Calling a list of registered voters: This captures active voters, but leaves out people who don’t put their phone number on their voter registration, as well as the unregistered new voters.</li>
<li>Random digit dialing: This captures a larger universe, but it relies on respondents to be truthful about their registration status.</li>
<li>Using online polls: This… is cheap. Phone response rates are <a href="https://www.pewresearch.org/fact-tank/2019/02/27/response-rates-in-telephone-surveys-have-resumed-their-decline/">pretty low these days</a> at 6%, which means more dialing, which means surveys get costlier. Also, respondents are more likely to be truthful online, so this method avoids the <a href="https://en.wikipedia.org/wiki/Bradley_effect">Bradley effect</a>. However, being a nascent method, their <a href="https://www.nytimes.com/2019/07/02/upshot/online-polls-analyzing-reliability.html">accuracy is questionable</a>.</li>
</ol>
<p><strong>House effect.</strong> This is the systematic over-representation of Democratic or Republican respondents. Here are a couple of examples:</p>
<ul>
<li><p>The USC/L.A. Times polls for state races leaned democrat in 2010 because they used <a href="https://fivethirtyeight.com/features/when-house-effects-become-bias/">bilingual interviewers</a></p></li>
<li><p>Many polls over-estimated Clinton’s support in 2016 because college graduates, who by 2016 were generally left of the aisle, are <a href="https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx">more likely to take surveys</a></p></li>
</ul>
<p><strong>Weights.</strong> The above problems can theoretically be corrected by applying demographic weights, but this requires making assumptions that could be faulty.</p>
</div>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Tidy Tuesday: Avoiding Dual Axis Plots</title>
            <link rel="alternate" type="text/html" href="/post/01-29-2019-usda-post/" />
            <id>/post/01-29-2019-usda-post/</id>
            <updated>2020-01-04T19:53:07-05:00</updated>
            <published>2019-11-05T00:00:00+00:00</published>
            <author>
                    <name>Maggie Church</name>
                    <uri>/</uri>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html"><![CDATA[Visualizing Two Variables of Interest The Prompt For my first #tidytuesday, I chose an old prompt. I wanted to explore USDA milk consumption data. The accompanying NPR article considers the oversupply of cheese in the US. Andrew Novakovic, an agricultural economist at Cornell University, explained that milk production has risen while consumption has fallen, so suppliers turn the milk into cheese which is less perishable.]]></summary>
            
                <content type="html"><![CDATA[


<div id="visualizing-two-variables-of-interest" class="section level1">
<h1>Visualizing Two Variables of Interest</h1>
<div id="the-prompt" class="section level2">
<h2>The Prompt</h2>
<p>For my first #tidytuesday, I chose an old prompt. I wanted to explore USDA milk consumption data. The accompanying <a href="https://www.npr.org/2019/01/09/683339929/nobody-is-moving-our-cheese-american-surplus-reaches-record-high">NPR article</a> considers the oversupply of cheese in the US. Andrew Novakovic, an agricultural economist at Cornell University, explained that milk production has risen while consumption has fallen, so suppliers turn the milk into cheese which is less perishable. But Americans are turning to less processed/more expensive cheeses, and consuming cheese overall.</p>
<p>Since I was late to this prompt, I was able to check out what others had already done with it. <a href="https://twitter.com/Alex_Danvers/status/1090496421160075264">@Alex_Danvers</a> compared milk production to google search trends for “lactose,” which I thought was interesting. I decided to do the same, but for milk <em>consumption</em>, which I thought was more relevant.</p>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>The USDA dairy consumption data has yearly domestic consumption for milk, yogurt, butter, etc. in lbs per person, between 1975-2018. I’m just going to focus on milk.</p>
<pre><code>## # A tibble: 6 x 8
##    year  milk yogurt butter american_cheese other_cheese cottage_cheese
##   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;
## 1  1975   247    2      4.7             8.1          6.1            4.6
## 2  1976   247    2.1    4.3             8.9          6.6            4.6
## 3  1977   244    2.3    4.3             9.2          6.8            4.6
## 4  1978   241    2.4    4.4             9.5          7.3            4.6
## 5  1979   238    2.4    4.5             9.6          7.6            4.4
## 6  1980   234    2.5    4.5             9.6          7.9            4.4
## # ... with 1 more variable: ice_cream &lt;dbl&gt;</code></pre>
<p>The Google Trends data is monthly, and unfortunately only goes back to 2004. Values represent search interest relative to the highest point on the chart for U.S. searches between 2004 and 2018. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular as when it peaked. In this data, the term peaked in July of 2018.</p>
<pre><code>## # A tibble: 6 x 2
##   Month   `lactose: (United States)`
##   &lt;chr&gt;                        &lt;int&gt;
## 1 2004-01                         42
## 2 2004-02                         41
## 3 2004-03                         51
## 4 2004-04                         43
## 5 2004-05                         42
## 6 2004-06                         41</code></pre>
<p>Since the USDA data is yearly, I find the yearly average of lactose trends and merge the datasets on year.</p>
</div>
</div>
<div id="the-plot" class="section level1">
<h1>The Plot</h1>
<div id="dual-axis-plot" class="section level3">
<h3>Dual Axis Plot</h3>
<p>First, I plotted both of the variables I was interested in (milk consumption and google search trends) over time. Here’s the graph using raw data. We can see that there’s a spike in search popularity between 2010-2012 and 2014-2018, and a steep decrease in milk consumption between 2010-2014. These two variables definitely appear to be negatively correlated, and indeed their correlation coefficient is -0.98, but is this the best way to demonstrate their relationship?</p>
<p><img src="/post/01-29-2019-USDA-post_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="side-by-side-plot" class="section level3">
<h3>Side-by-Side Plot</h3>
<p>My partner Andrew convinced me that dual axis plots are generally bad practice. Lisa Charlotte Ross <a href="https://blog.datawrapper.de/dualaxis/">wrote a great post</a> about why that is. Basically, they can be misleading about relationships.</p>
<p>One of Lisa’s suggestions was to use side-by-side plots. This doesn’t really offer any more information, but it does keep the reader from making those subconscious false assumptions due to dual axes.</p>
<p><img src="/post/01-29-2019-USDA-post_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="labeled-scatterplot" class="section level3">
<h3>Labeled scatterplot</h3>
<p>Another of Lisa’s suggestions was to create a Labeled scatterplot instead. This is nice because it shows us the relationship between our variables of interest, without excluding the year, which itself contains a lot of implicit info.</p>
<p>Below, I see that there is a negative relationship between lactose search trends and milk consumption. We also see that lactose searches increase over time, and milk consumption decreases over time. However, I liked having year on the x-axis. I think it’s more intuitive, and I like seeing each variable’s slope over time.</p>
<p><img src="/post/01-29-2019-USDA-post_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="indexed-plot-standardized" class="section level3">
<h3>Indexed Plot (Standardized)</h3>
<p>One last suggestion of Lisa’s was to make an indexed plot. That is, adjust the scales of our two data series and compare them on one common scale.</p>
<p>I index first by standardizing. This is what <span class="citation">@Alex_Danvers</span> did in the first place. Here I’m essentially just rescaling the vertical axes into relative terms. Notice all that space between these two lines in the original graph? Well, now I can zoom in on the action. Yes, I lose the information that the absolute values tell us, but I’m more interested in the relative changes anyways.</p>
<p>The y-axis here will be z-score. In case you forgot Stat 101, the z-score indicates how much a given value differs from the standard deviation. For example, I can see that in 2006, milk consumption was 1 standard deviation above the mean consumption between 2004-2018.</p>
<p>Below, I can see the same trends that I saw in my first graph, but more clearly. There’s that spike in search popularity between 2010-2012 and 2014-2018, and that big fall in milk consumption between 2010-2014. As it should be, the correlation coefficient of milk consumption and lactose search popularity is the same as before, -0.98.</p>
<p><img src="/post/01-29-2019-USDA-post_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Note that when <span class="citation">@Alex_Danvers</span> made this graph using milk <em>production</em> instead of consumption, he found a weaker relationship with lactose search popularity. This makes sense to me.</p>
<p>To like this graph, you have to understand z-scores, which is a bit esoteric. But I do like the way it allows me to effectively compare the two series on a common scale</p>
</div>
<div id="indexed-plot-change" class="section level3">
<h3>Indexed Plot (% Change)</h3>
<p>Next, I index using percent changes instead. That works well here because both of my variables of interest have similar rates of change, so I can easily see what’s going on with each.</p>
<p>Below, I can see that milk consumption declined in each year in our time period, and lactose searches increase for all but one year. When google searches for lactose experienced a big jump However, when “lactose” searches experienced another jump in 2017, this time milk consumption <em>doesn’t</em> fall hard. Thus, it doesn’t seem like the changes in lactose searches and the changes in milk consumption are very related. As it turns out, the correlation coefficient this time is -0.13.</p>
<p><img src="/post/01-29-2019-USDA-post_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>I like this last plot the best, because it holds the most interesting story. While all of the other graphs showed that lactose searches go up during the same time that milk consumption goes down, this is the only graph that suggests the spikes aren’t necessarily related.</p>
</div>
</div>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">AT Visualized</title>
            <link rel="alternate" type="text/html" href="/post/at_data_analysis/" />
            <id>/post/at_data_analysis/</id>
            <updated>2020-01-04T19:53:15-05:00</updated>
            <published>2019-11-04T00:00:00+00:00</published>
            <author>
                    <name>Maggie Church</name>
                    <uri>/</uri>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html"><![CDATA[In 2019, I hiked northbound on the Appalachian Trail, 2,192 from Georgia to Maine. It took 170 days. After I finished, I went through my journal and guidebook to create a spreadsheet with each day’s mileage, location, sleep site, and other notes.]]></summary>
            
                <content type="html"><![CDATA[
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<p>In 2019, I hiked northbound on the Appalachian Trail, 2,192 from Georgia to Maine. It took 170 days. After I finished, I went through my journal and guidebook to create a spreadsheet with each day’s mileage, location, sleep site, and other notes.</p>
<p>Below, I visualize my mileage and attempt to find trends. I also breakdown my sleep sites by type.</p>
<p><br></p>
<p><img src="/post/AT_data_analysis_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Clearly, my mileage was all over the place. I started out relatively strong. I began to really push my mileage in VA. My pace slowed a bit in PA, probably due to the rocks and heat. Between NJ and VT, I changed my hiking style, hiking fewer miles but more consistently. I slowed significantly in NH and southern ME because of the difficult terrain, and picked back up in the Hundred Mile Wilderness.</p>
<p>My average mileage was 12.89 if I include zero days and 14.52 if I exclude them.</p>
<p>My biggest day was 31 miles.</p>
<p>I took a total of 19 zero days: 8 were on-trail (this includes days spent in trail towns), 8 were off-trail, and 3 were for Trail Days.</p>
<p>The average number of days that I hiked before taking a zero day was 7.58.</p>
<p>My longest stretch without taking a zero day was 24 days, between NJ and VT.</p>
<p><br></p>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"data":[{"labels":["outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","indoors","outdoors","outdoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","indoors","outdoors","indoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","indoors","indoors","indoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","indoors","indoors","indoors","outdoors","indoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","outdoors","indoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors","indoors","indoors","outdoors","outdoors","outdoors","indoors","indoors","outdoors","indoors","indoors","indoors","indoors","indoors","indoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","indoors","outdoors","outdoors","indoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","indoors","indoors","outdoors","outdoors","outdoors","outdoors","outdoors","indoors"],"showlegend":true,"textinfo":"none","type":"pie","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(255,255,255,1)"}},"frame":null},{"labels":["Yard","Hostel","Hotel","Airbnb","Home","AMC Hut","Campsite","Paid Campsite","Shelter","Stealth"],"values":[8,17,16,5,12,5,3,5,71,28],"marker":{"color":"rgba(255,127,14,1)","colors":["#0ec3ff","#f7b6a6","#f7cea8","#e8b4a7","#fcd19f","#ffaea3","#c7dbfc","#a5c4fa","#c2c6fc","#709ae0"],"line":{"color":"rgba(255,255,255,1)"}},"sort":false,"showlegend":false,"textinfo":"none","type":"pie","hole":0.6,"frame":null}],"layout":{"NA":{"anchor":[],"domain":[0,1]},"NA2":{"anchor":[],"domain":[0,1]},"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":true,"title":"Site Types (this is interactive, so hover around!)","xaxis":{"showgrid":false,"zeroline":false,"showticklabels":false},"yaxis":{"showgrid":false,"zeroline":false,"showticklabels":false}},"attrs":{"68c071a84555":{"labels":{},"showlegend":true,"textinfo":"none","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"pie"},"68c04f66fa04":{"labels":{},"values":{},"marker":{"colors":["#0ec3ff","#f7b6a6","#f7cea8","#e8b4a7","#fcd19f","#ffaea3","#c7dbfc","#a5c4fa","#c2c6fc","#709ae0"]},"sort":false,"showlegend":false,"textinfo":"none","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"pie","hole":0.6,"inherit":true}},"source":"A","config":{"showSendToCloud":false},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"subplot":true,"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The plurality win goes to shelters. I slept in or tented near shelters due to their amenities: large tenting space, a roof, privy, bear box.</p>
<p>I slept outdoors for only 67% of the nights. In retrospect, I didn’t realize I had slept indoors so often, but it didn’t feel more than other hikers…</p>
<p>In terms of paid lodging, I spent 38 nights in either a hotel, hostel, Airbnb, or paid campsite. 7 of those hostels were donation-based barns or churchs. 3 of the paid campsites were for Trail Days.</p>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>